{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "            dbname=\"dockertestdb\",\n",
    "            user=\"manav1011\",\n",
    "            password=\"Manav@1011\",\n",
    "            host=\"192.168.7.70\",\n",
    "            port=5432\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT username,transcript,created_at \n",
    "    FROM zoom.transcripts \n",
    "    WHERE meeting_id = %s \n",
    "    ORDER BY created_at ASC;\n",
    "\"\"\"\n",
    "\n",
    "cur.execute(query, (\"9FZahlbwQpmPU2bNTRUSkw==\",))\n",
    "records = cur.fetchall()\n",
    "\n",
    "# Format results as JSON\n",
    "transcripts = [\n",
    "    {\n",
    "        \"username\": row[0],\n",
    "        \"transcript\": row[1],\n",
    "        \"timestamp\":row[2]\n",
    "    }\n",
    "    for row in records\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make whole conversation\n",
    "conversation = ''\n",
    "for transcript in transcripts:\n",
    "    text = f\"{transcript['timestamp']} - {transcript['username']} : {transcript['transcript']} \\n\"\n",
    "    conversation+=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text, chunk_size=5000, tolerance=1000):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "\n",
    "        # If there's more text left and we haven't exceeded the tolerance\n",
    "        if end < len(text) and (end + tolerance) < len(text):\n",
    "            end = text.rfind(' ', start, end)  # Try to split at a space\n",
    "            if end == -1:  \n",
    "                end = min(start + chunk_size, len(text))  # Fallback to hard cut\n",
    "\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the chunking of 5000 tokens each\n",
    "chunks = split_into_chunks(conversation)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/web-h-054/Documents/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyD0vLS27RzUfBTiFb3lv4tbxsS10BL3Sio\")\n",
    "text_model = genai.GenerativeModel(\"gemini-1.5-flash-8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def query_llm(chunk):\n",
    "    \"\"\"\n",
    "    Sends conversation chunks to the LLM for summarization, ensuring context is preserved \n",
    "    even if the order of user turns varies.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine chunks into a single context strin\n",
    "\n",
    "    # Construct system and user messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert conversation summarizer.\"},\n",
    "        {\"role\": \"system\", \"content\": \"Your goal is to provide a coherent summary, even if the user speaking order is mixed.\"},\n",
    "        {\"role\": \"system\", \"content\": \"Maintain conversational flow, capture key details, and avoid redundancy.\"},\n",
    "        {\"role\": \"system\", \"content\": \"temperature: 0.7\"},  # Higher temperature for better context inference\n",
    "        {\"role\": \"system\", \"content\": \"Respond strictly in JSON format. Do NOT use markdown formatting or code blocks. Ensure the response contains only the JSON object and nothing else.\"},\n",
    "        {\"role\": \"user\", \"content\": chunk}\n",
    "    ]\n",
    "\n",
    "    # Generate response from LLM\n",
    "    response = text_model.generate_content(json.dumps(messages))\n",
    "\n",
    "    # Extract JSON from response\n",
    "    match = re.search(r\"\\{.*\\}\", response.text, re.DOTALL)  # Matches curly braces and content inside\n",
    "    if match:\n",
    "        json_string = match.group(0)\n",
    "    else:\n",
    "        print(\"No JSON found in response.\")\n",
    "        return None  \n",
    "\n",
    "    # Parse JSON safely\n",
    "    try:\n",
    "        data = json.loads(json_string)\n",
    "        return data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        print(f\"Raw response: {json_string}\")  # Debugging info\n",
    "        return None  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Manav and Nikhil reconnected after a long time. Nikhil had just returned from a two-week trip to Spain, describing it as incredible, highlighting the food and architecture.  Manav mentioned his busy work schedule.  They discussed specific locations in Spain, traditional Spanish dishes (like tapas), and Nikhil's impressions of the Spanish culture and architecture.  Manav mentioned wanting to try recreating some Spanish food.  They also discussed Manav's recent work, including a significant product launch.  The conversation flowed from a general catch-up to specific details about their experiences, and ended with an inquiry about Manav's work.\",\n",
       " \"Nikhil and Manav discussed recent events. Nikhil shared about a stressful cloud platform update, and a new YouTube channel focused on travel vlogs, having recorded footage in Spain. Manav commented on the platform update, mentioned his own Netflix/gaming/hiking routine (with some guitar learning), and expressed enthusiasm for Nikhil's YouTube channel.  They both agreed to stay in touch and potentially meet again next week.\",\n",
       " \"Nikhil Maheshwari and Manav Shah are discussing a meeting. Nikhil is preparing for a meeting in five minutes and wants to know when they'll meet in person. Manav mentions catching up regarding the grid. Nikhil has a follow-up question about something not fully clear.\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = []\n",
    "for chunk in chunks:\n",
    "    response = query_llm(chunk)\n",
    "    summaries.append(response['summary'])\n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge summary for next process\n",
    "def generate_final_summary(summaries, recursion_depth=0, max_recursion=10):\n",
    "    print(f\"Recursion Depth : {recursion_depth}\")\n",
    "    final_summary = ''\n",
    "    if len(summaries) == 1:\n",
    "        final_summary = summaries[0]\n",
    "        return final_summary\n",
    "\n",
    "    if recursion_depth >= max_recursion:\n",
    "        combined_summary = ' '.join(summaries)\n",
    "        response = query_llm([combined_summary])\n",
    "        return response['summary']\n",
    "\n",
    "    concated_summary = ' '.join(summaries)\n",
    "    sub_summaries = split_into_chunks(concated_summary)\n",
    "    if len(sub_summaries) == 1:\n",
    "        response = query_llm(sub_summaries[0])\n",
    "        final_summary = response['summary']\n",
    "        return final_summary\n",
    "    new_summaries = []\n",
    "    for summary in sub_summaries:\n",
    "        response = query_llm(summary)\n",
    "        new_summaries.append(response['summary'])\n",
    "    return generate_final_summary(new_summaries, recursion_depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursion Depth : 0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "final_summary = generate_final_summary(summaries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
